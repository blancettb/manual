{
  "name": "Node at capacity limits",
  "description": "This rule checks for the Elasticsearch node being at the capacity limit which is determined by the presence of the following issues: high load and CPU usage on the host, high heap usage and high GC time in the Elasticsearch JVM.",
  "type": "MANUAL",
  "tags": [
     ""
  ],
  "fields": [
    {
        "name": "content",
        "description": "Content for manual action",
        "encoding": "base64",
        "value": "
1. Check the current resource utilization of your Elasticsearch nodes using tools like `top`, `htop`, or `sar`. These commands will help you identify if any of the nodes are experiencing high CPU usage, memory consumption, or other performance issues.
2. If you find that one or more nodes are indeed overloaded, consider increasing the size of the JVM heap for those nodes. You can do this by adding the following configuration to the Elasticsearch configuration file:

   ```
   xpack.watcher.node_memory_high_watermark_exceeded.threshold: "90%"
   xpack.watcher.node_memory_low_watermark_exceeded.threshold: "70%"
   ```

   This will set a higher threshold for the memory usage alerts, giving you more time to address the issue before it becomes critical.
3. Another option is to adjust the number of indexing threads on the overloaded nodes. By default, Elasticsearch uses 10 concurrent indexing threads per node. If you have fewer than 20 nodes, you can increase this value to 20-30 threads. However, be cautious not to overload the system with too many threads as it may lead to performance degradation.
4. Lastly, if you're running into issues with disk space, you can increase the disk space allocated to each node. To do this, modify the `elasticsearch.yml` file and add the following lines under the `node.data` section:

   ```
   path_data: /var/lib/elasticsearch/data
   path_logs: /var/log/elasticsearch/
   ```

   Make sure to restart the Elasticsearch service for these changes to take effect.

By following these steps, you should be able to resolve the Node at capacity limits issue and ensure optimal performance for your Elasticsearch environment.",
        "secured": false
    }
],
  "staticId": "containerCPUDebug",
  "metadata": {
    "ai": [
      {
        "algorithm": "watsonx",
        "events": ["ssLy1L0rXUze74XOaw_2coB2rGQ"]
      }
    ]
  }
}